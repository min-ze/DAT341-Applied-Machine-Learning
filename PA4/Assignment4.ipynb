{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 4  \n",
    "Minze Teh & Sebastian KÃ¶lbel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by testing the perceptron class provided in the assignment, and we were able to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "class Perceptron(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # Perceptron algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <= 0:\n",
    "                    self.w += y*x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.21 sec.\n",
      "Accuracy: 0.7919.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('DAT341-Applied-Machine-Learning/PA4/data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    \n",
    "    Perceptron()\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to run the code and recieved an accuracy of just under 0.8. We moved on to look at the Pegasus algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    An implementation of the Pegasus learning algorithm using the SVC approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter  = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "    \n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        lambda_ = 1/n_samples\n",
    "\n",
    "        # Pegasus algorithm:\n",
    "        for t in range(1,self.n_iter+1):\n",
    "\n",
    "            idx = np.random.randint(n_samples)\n",
    "\n",
    "            eta = 1/(lambda_*t)\n",
    "\n",
    "            x, y = X[idx], Ye[idx]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = self.w.dot(x)\n",
    "            # If there was an error, update the weights.\n",
    "            self.w *= 1-eta*lambda_\n",
    "            \n",
    "            if y*score < 1:\n",
    "                self.w += x*eta*y\n",
    "\n",
    "class PegasusLR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    An implementation of the pegasus learning algorithm using LR approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter  = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "    \n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        lambda_ = 1/n_samples\n",
    "\n",
    "        # Pegasus algorithm:\n",
    "        for t in range(1,self.n_iter+1):\n",
    "\n",
    "            idx = np.random.randint(n_samples)\n",
    "\n",
    "            eta = 1/(lambda_*t)\n",
    "\n",
    "            x, y = X[idx], Ye[idx]\n",
    "            \n",
    "            # calculate gradient of log Loss\n",
    "            dLoss = -y/(1+np.exp(y*(self.w*x)))\n",
    "            # calc gradient\n",
    "            gradient = lambda_*self.w+dLoss\n",
    "            # update step\n",
    "            self.w -= eta*gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.13 sec.\n",
      "Accuracy: 0.8133.\n"
     ]
    }
   ],
   "source": [
    "# Read all the documents.\n",
    "X, Y = read_data('DAT341-Applied-Machine-Learning/PA4/data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    \n",
    "    PegasusSVC(n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.13 sec.\n",
      "Accuracy: 0.4927.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    \n",
    "    PegasusLR(n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression has a lower accuracy compared to linearSVC as they utilize different optimization function. Logistic regression minimizes the log loss function  which models class probabilities directly, while linear SVC minimizes the hinge loss function,  maximising the margin between classes. This difference becomes significant when dealing with data that is not linearly separable, as logistic regression may struggle to find an appropriate decision boundary. Apart from this, logistic regression may be sensitive to class imbalances, potentially leading to biased predictions if one class is more apparent in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continued to optimize the algorithm using the BLAS functions as well as a approach using sparse vector operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg.blas as blas\n",
    "\n",
    "class PegasusSVC_BLAS(LinearClassifier):\n",
    "    \"\"\"\n",
    "    An implementation of the pegasus learning algorithm using SVC and BLAS functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter  = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasus learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "    \n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "        lambda_ = 1/n_samples\n",
    "\n",
    "        # Pegasus algorithm:\n",
    "        for t in range(1,self.n_iter+1):\n",
    "\n",
    "            idx = np.random.randint(n_samples)\n",
    "\n",
    "            eta = 1/(lambda_*t)\n",
    "\n",
    "            x, y = X[idx], Ye[idx]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = blas.ddot(self.w,x)\n",
    "\n",
    "            # If there was an error, update the weights.\n",
    "            \n",
    "            blas.dscal(1-eta*lambda_, self.w)\n",
    "            if y*score < 1:\n",
    "                \n",
    "                blas.daxpy(x, self.w, a=eta*y)\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "class SparsePegasusSVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm,\n",
    "    assuming that the input feature matrix X is sparse.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "         \n",
    "        # set lambda\n",
    "        lambda_ = 1/n_samples\n",
    "\n",
    "        # pegasus algorothm\n",
    "        for t in range(1,self.n_iter+1):\n",
    "\n",
    "            idx = np.random.randint(n_samples)\n",
    "\n",
    "            eta = 1/(lambda_*t)\n",
    "\n",
    "            x, y = X[idx], Ye[idx]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = sparse_dense_dot(x,self.w)\n",
    "\n",
    "            # If there was an error, update the weights.\n",
    "            self.w *= 1-eta*lambda_\n",
    "            if y*score < 1:\n",
    "                add_sparse_to_dense(x, self.w, eta*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 17.50 sec.\n",
      "Accuracy: 0.8258.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    PegasusSVC(n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "train_time = t1-t0\n",
    "print('Training time: {:.2f} sec.'.format(train_time))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 11.06 sec.\n",
      "Accuracy: 0.8405.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    PegasusSVC_BLAS(n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "train_time_blas = t1-t0\n",
    "print('Training time: {:.2f} sec.'.format(train_time_blas))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.60 sec.\n",
      "Accuracy: 0.8485.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    SparsePegasusSVC(n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "train_time_sparse = t1-t0\n",
    "print('Training time: {:.2f} sec.'.format(train_time_sparse))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparsePegasusSVC2(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=20):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features)\n",
    "         \n",
    "        # set lambda\n",
    "        self.lambda_ = 1/n_samples\n",
    "        a = 1\n",
    "        # pegasus algorithm\n",
    "        for t in range(1,self.n_iter+1):\n",
    "\n",
    "            idx = np.random.randint(n_samples)\n",
    "\n",
    "            # For this approach the eta was modified slightly to prevent a from becoming 0.\n",
    "            eta = 1.000001/(self.lambda_*t)\n",
    "\n",
    "            x, y = X[idx], Ye[idx]\n",
    "\n",
    "            # scaling step\n",
    "            a *= 1-eta*self.lambda_\n",
    "                  \n",
    "            # Compute the output score for this instance.\n",
    "            score = a * sparse_dense_dot(x,self.w)\n",
    "\n",
    "            if y*score < 1: \n",
    "                add_sparse_to_dense(x, self.w, eta*y/a)\n",
    "                \n",
    "        # use accumulated factor to adjust the weights\n",
    "        self.w *= a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 3.54 sec.\n",
      "Accuracy: 0.8410.\n"
     ]
    }
   ],
   "source": [
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    SparsePegasusSVC2(n_iter=10000)\n",
    ")\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "train_time_sparse_2 = t1-t0\n",
    "print('Training time: {:.2f} sec.'.format(train_time_sparse_2))\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Training times in sec\n",
       "   |Pre-optimization|BLAS|Sparse|Sparse (scaling factor)|\n",
       "   |--|--|--|--|\n",
       "   |17.50|11.06|5.60|3.54| "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(\"\"\"Training times in sec\n",
    "   |Pre-optimization|BLAS|Sparse|Sparse (scaling factor)|\n",
    "   |--|--|--|--|\n",
    "   |%.2f|%.2f|%.2f|%.2f| \"\"\" %(train_time,train_time_blas,train_time_sparse,train_time_sparse_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is visible in the table that the optimization steps performed yielded a shorter training time. with the sparse approach being the most time saving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
