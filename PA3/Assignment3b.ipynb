{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3b Sentiment Classification\n",
    "#### Applied Machine Learning\n",
    "Grpup 39: Sebastian KÃ¶lbel & Min Ze Teh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing libraries needed for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# for converting training and test datasets into matrices\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function for preprocessing the csv-files. Thes preprocessing steps include:\n",
    "* Making text all lower case\n",
    "* Removing leading and closing white-space\n",
    "* Make sure punctuations are separated from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiments(doc_file):\n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        new_lines = []\n",
    "        for line in f:\n",
    "            line = line.lower().removesuffix('\\n').strip().replace('.',' . ')\n",
    "            new_lines.append(line.split('\\t'))\n",
    "                \n",
    "        return pd.DataFrame(new_lines,columns=['sentiment','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then separate the input values from the output values. The crowdsourced data needed some more preprocessing. There we too many values for the sentiments all consisting of typos. We checked how many errors there were, however there were only 75 errors out of more than 10 000 rows so the errors were dropped to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows in crowdsourced data: 75\n"
     ]
    }
   ],
   "source": [
    "df = read_sentiments('Data/crowdsourced_train.csv').drop(0)\n",
    "cs_training = df[df['sentiment'].isin(['positive','negative','neutral'])]\n",
    "print('Dropped rows in crowdsourced data:',len(df)-len(cs_training))\n",
    "X_cs_train = cs_training.drop('sentiment', axis=1)\n",
    "Y_cs_train = cs_training['sentiment']\n",
    "\n",
    "gold_training = read_sentiments('Data/gold_train.csv').drop(0)\n",
    "X_gold_train = gold_training.drop('sentiment', axis=1)\n",
    "Y_gold_train = gold_training['sentiment']\n",
    "\n",
    "\n",
    "testing = read_sentiments('Data/test.csv').drop(0)\n",
    "X_test = testing.drop('sentiment', axis=1)\n",
    "Y_test = testing['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline( TfidfVectorizer(), LinearSVC(dual='auto') )\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
