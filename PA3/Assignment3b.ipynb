{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA3b Sentiment Classification\n",
    "#### Applied Machine Learning\n",
    "Grpup 39: Sebastian KÃ¶lbel & Min Ze Teh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing libraries needed for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# for converting training and test datasets into matrices\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function for preprocessing the csv-files. These preprocessing steps include:\n",
    "* Making text all lower case\n",
    "* Removing leading and closing white-space\n",
    "* Make sure punctuations are separated from words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentiments(doc_file):\n",
    "    with open(doc_file, 'r', encoding='utf-8') as f:\n",
    "        new_lines = []\n",
    "        for line in f:\n",
    "            line = line.lower().removesuffix('\\n').strip().replace()\n",
    "            new_lines.append(line.split('\\t'))\n",
    "                \n",
    "        return pd.DataFrame(new_lines,columns=['sentiment','text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then separate the input values from the output values. The crowdsourced data needed some more preprocessing. There we too many values for the sentiments all consisting of typos. We checked how many errors there were, however there were only 75 errors out of more than 10 000 rows so the errors were dropped to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows in crowdsourced data: 75\n"
     ]
    }
   ],
   "source": [
    "df = read_sentiments('Data/crowdsourced_train.csv').drop(0)\n",
    "cs_training = df[df['sentiment'].isin(['positive','negative','neutral'])]\n",
    "print('Dropped rows in crowdsourced data:',len(df)-len(cs_training))\n",
    "X_cs_train = cs_training.drop('sentiment', axis=1)\n",
    "Y_cs_train = cs_training['sentiment']\n",
    "\n",
    "gold_training = read_sentiments('Data/gold_train.csv').drop(0)\n",
    "X_gold_train = gold_training.drop('sentiment', axis=1)\n",
    "Y_gold_train = gold_training['sentiment']\n",
    "\n",
    "\n",
    "testing = read_sentiments('Data/test.csv').drop(0)\n",
    "X_test = testing.drop('sentiment', axis=1)\n",
    "Y_test = testing['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_classifier(X, Y):\n",
    "    pipeline = make_pipeline(\n",
    "        # punctuations are removed here, max df should make sure that no too common words are present\n",
    "        TfidfVectorizer(encoding='utf-8',  \n",
    "                        max_df=0.05), \n",
    "        BernoulliNB())\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_classifier2(X, Y):\n",
    "    pipeline = make_pipeline(\n",
    "        # punctuations are removed here, max df should make sure that no too common words are present\n",
    "        TfidfVectorizer(encoding='utf-8', max_df=0.05), \n",
    "        LogisticRegression())\n",
    "    pipeline.fit(X, Y)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/koelbel/aml/my_project_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.58580007        nan 0.63844157        nan 0.63535037]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression parameters:  {'C': 1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "something1 = TfidfVectorizer(encoding='utf-8', max_df=0.05)\n",
    "something2 = something1.fit_transform(X_gold_train['text'])#train_document_classifier2(X_gold_train['text'], Y_gold_train)\n",
    "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "search = GridSearchCV(LogisticRegression(), param_grid)\n",
    "search.fit(something2, Y_gold_train)\n",
    "print('Logistic Regression parameters: ', search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[197], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      2\u001b[0m C_Matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(Ytest, Yguess, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAntivaccine\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvaccine\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "C_Matrix = confusion_matrix(Ytest, Yguess, normalize=\"true\")\n",
    "\n",
    "labels = [\"Antivaccine\", \"Provaccine\"]\n",
    "plt.figure(figsize=(18, 6))\n",
    "seaborn.heatmap(C_Matrix, annot=True, square=True, cmap=\"Reds\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"Actual label\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7119840695148443"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vec = TfidfVectorizer()\n",
    "clf = train_document_classifier(X_cs_train['text'], Y_cs_train)\n",
    "clf2 = train_document_classifier2(X_gold_train['text'], Y_gold_train)\n",
    "len(clf[0].stop_words_)\n",
    "#clf[1].\n",
    "\n",
    "#accuracy_score(Y_test, clf.predict(X_test['text']))\n",
    "accuracy_score(Y_test, clf2.predict(X_test['text']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
