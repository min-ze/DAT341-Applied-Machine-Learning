{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA1\n",
    "#### Applied Machine Learning\n",
    "Grpup 39: Sebastian KÃ¶lbel & Min Ze Teh\n",
    "## Task1\n",
    "We begin by using the code provided in the assignment to transform the data in the csv file into a panda dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "  \n",
    "# Read the CSV file.\n",
    "data = pd.read_csv('Data/CTG.csv', skiprows=1)\n",
    "\n",
    "# Select the relevant numerical columns.\n",
    "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
    "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
    "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
    "data = data[selected_cols].dropna()\n",
    "\n",
    "# Shuffle the dataset.\n",
    "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Split into input part X and output part Y.\n",
    "X = data_shuffled.drop('NSP', axis=1)\n",
    "\n",
    "# Map the diagnosis code to a human-readable label.\n",
    "def to_label(y):\n",
    "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
    "\n",
    "Y = data_shuffled['NSP'].apply(to_label)\n",
    "\n",
    "# Partition the data into training and test sets.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing three classifiers we decided to pick one classifier form each of the different sub categories. We picked gradient boosting classifier and attempted to optimize it in terms of max depth, trying out different depths we found that we got the best performance when having a max depth of 5 or 6.\n",
    "\n",
    "For the Multi-layer perceptron classfier we tried adjusting the hidden_layer_sizes and found that we got the best accuracy when using the input value 75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting classifier accuracy : 0.95\n",
      "Perceptron accuracy: 0.825294117647059\n",
      "MLP Classifier accuracy: 0.8841176470588236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "from sklearn.linear_model import Perceptron as perc\n",
    "from sklearn.neural_network import MLPClassifier as mlpc\n",
    "\n",
    "gbc_clf = gbc(max_depth=5,random_state=1)\n",
    "perc_clf = perc() \n",
    "mlpc_clf = mlpc(random_state= 1, hidden_layer_sizes=75)\n",
    "\n",
    "print('Gradient boosting classifier accuracy :', cross_val_score(gbc_clf, Xtrain, Ytrain).mean())\n",
    "print('Perceptron accuracy:', cross_val_score(perc_clf, Xtrain, Ytrain).mean())\n",
    "print('MLP Classifier accuracy:', cross_val_score(mlpc_clf, Xtrain, Ytrain).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final evaluation we picked the classifier that we found to have the best accuracy, namely the gradient boosting classifier. we fit the values and performed the prediction the resulting accuracy can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting classifier accuracy:  0.931924882629108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gbc_clf.fit(Xtrain, Ytrain)\n",
    "Yguess = gbc_clf.predict(Xtest)\n",
    "print('Gradient boosting classifier accuracy: ',accuracy_score(Ytest, Yguess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by copying the class' needed for the DecisionTree class. This was provided in the lectures notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeLeaf:\n",
    "\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
    "    def predict(self, x):\n",
    "        return self.value\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_id = str(node_counter)\n",
    "        val_str = f'{self.value:.4g}' if isinstance(self.value, float) else str(self.value)\n",
    "        graph.node(node_id, val_str, style='filled')\n",
    "        return node_counter+1, node_id\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, DecisionTreeLeaf):\n",
    "            return self.value == other.value\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeBranch:\n",
    "\n",
    "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.low_subtree = low_subtree\n",
    "        self.high_subtree = high_subtree\n",
    "\n",
    "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
    "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
    "    # than the threshold.\n",
    "    def predict(self, x):\n",
    "        if x[self.feature] <= self.threshold:\n",
    "            return self.low_subtree.predict(x)\n",
    "        else:\n",
    "            return self.high_subtree.predict(x)\n",
    "\n",
    "    # Utility function to draw a tree visually using graphviz.\n",
    "    def draw_tree(self, graph, node_counter, names):\n",
    "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
    "        node_id = str(node_counter)\n",
    "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
    "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
    "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
    "        graph.edge(node_id, low_id, 'False')\n",
    "        graph.edge(node_id, high_id, 'True')\n",
    "        return node_counter+1, node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class DecisionTree(ABC, BaseEstimator):\n",
    "\n",
    "    def __init__(self, max_depth):\n",
    "        super().__init__()\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
    "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
    "    # called make_tree (see below).\n",
    "    def fit(self, X, Y):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.names = X.columns\n",
    "            X = X.to_numpy()\n",
    "        elif isinstance(X, list):\n",
    "            self.names = None\n",
    "            X = np.array(X)\n",
    "        else:\n",
    "            self.names = None\n",
    "        Y = np.array(Y)        \n",
    "        self.root = self.make_tree(X, Y, self.max_depth)\n",
    "        \n",
    "    def draw_tree(self):\n",
    "        graph = Digraph()\n",
    "        self.root.draw_tree(graph, 0, self.names)\n",
    "        return graph\n",
    "    \n",
    "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
    "    # for a set of instances.\n",
    "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "        return [self.predict_one(x) for x in X]\n",
    "\n",
    "    # Predicting the output for one instance.\n",
    "    def predict_one(self, x):\n",
    "        return self.root.predict(x)        \n",
    "\n",
    "    # This is the recursive training \n",
    "    def make_tree(self, X, Y, max_depth):\n",
    "\n",
    "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
    "        # For classifiers, this will be the most common value in Y.\n",
    "        default_value = self.get_default_value(Y)\n",
    "\n",
    "        # First the two base cases in the recursion: is the training set completely\n",
    "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
    "\n",
    "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
    "        if max_depth == 0:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # If all the instances in the remaining training set have the same output value,\n",
    "        # return a leaf with this value.\n",
    "        if self.is_homogeneous(Y):\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
    "        # we use one of the classification or regression criteria.\n",
    "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
    "        n_features = X.shape[1]\n",
    "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
    "        \n",
    "        if best_feature is None:\n",
    "            return DecisionTreeLeaf(default_value)\n",
    "\n",
    "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
    "        # the threshold or not\n",
    "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
    "\n",
    "        # Build the subtrees using a recursive call. Each subtree is associated\n",
    "        # with a value of the feature.\n",
    "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
    "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
    "\n",
    "        if low_subtree == high_subtree:\n",
    "            return low_subtree\n",
    "\n",
    "        # Return a decision tree branch containing the result.\n",
    "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
    "    \n",
    "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
    "    # and a threshold.\n",
    "    def split_by_feature(self, X, Y, feature, threshold):\n",
    "        low = X[:,feature] <= threshold\n",
    "        high = ~low\n",
    "        return X[low], X[high], Y[low], Y[high]\n",
    "    \n",
    "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_default_value(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_homogeneous(self, Y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def best_split(self, X, Y, feature):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
    "        super().__init__(max_depth)\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        # For decision tree classifiers, there are some different ways to measure\n",
    "        # the homogeneity of subsets.\n",
    "        if self.criterion == 'maj_sum':\n",
    "            self.criterion_function = majority_sum_scorer\n",
    "        elif self.criterion == 'info_gain':\n",
    "            self.criterion_function = info_gain_scorer\n",
    "        elif self.criterion == 'gini':\n",
    "            self.criterion_function = gini_scorer\n",
    "        else:\n",
    "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
    "        super().fit(X, Y)\n",
    "        self.classes_ = sorted(set(Y))\n",
    "\n",
    "    # Select a default value that is going to be used if we decide to make a leaf.\n",
    "    # We will select the most common value.\n",
    "    def get_default_value(self, Y):\n",
    "        self.class_distribution = Counter(Y)\n",
    "        return self.class_distribution.most_common(1)[0][0]\n",
    "    \n",
    "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
    "    # this means that all output values are identical.\n",
    "    # We assume that we called get_default_value just before, so that we can access\n",
    "    # the class_distribution attribute. If the class distribution contains just one item,\n",
    "    # this means that the set is homogeneous.\n",
    "    def is_homogeneous(self, Y):\n",
    "        return len(self.class_distribution) == 1\n",
    "        \n",
    "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
    "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
    "    # In the end, we return a triple consisting of\n",
    "    # - the best score we found, according to the criterion we're using\n",
    "    # - the id of the feature\n",
    "    # - the threshold for the best split\n",
    "    def best_split(self, X, Y, feature):\n",
    "\n",
    "        # Create a list of input-output pairs, where we have sorted\n",
    "        # in ascending order by the input feature we're considering.\n",
    "        sorted_indices = np.argsort(X[:, feature])        \n",
    "        X_sorted = list(X[sorted_indices, feature])\n",
    "        Y_sorted = list(Y[sorted_indices])\n",
    "\n",
    "        n = len(Y)\n",
    "\n",
    "        # The frequency tables corresponding to the parts *before and including*\n",
    "        # and *after* the current element.\n",
    "        low_distr = Counter()\n",
    "        high_distr = Counter(Y)\n",
    "\n",
    "        # Keep track of the best result we've seen so far.\n",
    "        max_score = -np.inf\n",
    "        max_i = None\n",
    "\n",
    "        # Go through all the positions (excluding the last position).\n",
    "        for i in range(0, n-1):\n",
    "\n",
    "            # Input and output at the current position.\n",
    "            x_i = X_sorted[i]\n",
    "            y_i = Y_sorted[i]\n",
    "            \n",
    "            # Update the frequency tables.\n",
    "            low_distr[y_i] += 1\n",
    "            high_distr[y_i] -= 1\n",
    "\n",
    "            # If the input is equal to the input at the next position, we will\n",
    "            # not consider a split here.\n",
    "            #x_next = XY[i+1][0]\n",
    "            x_next = X_sorted[i+1]\n",
    "            if x_i == x_next:\n",
    "                continue\n",
    "\n",
    "            # Compute the homogeneity criterion for a split at this position.\n",
    "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
    "\n",
    "            # If this is the best split, remember it.\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_i = i\n",
    "\n",
    "        # If we didn't find any split (meaning that all inputs are identical), return\n",
    "        # a dummy value.\n",
    "        if max_i is None:\n",
    "            return -np.inf, None, None\n",
    "\n",
    "        # Otherwise, return the best split we found and its score.\n",
    "        split_point = 0.5*(X_sorted[max_i] + X_sorted[max_i+1])\n",
    "        return max_score, feature, split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
    "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
    "    return maj_sum_low + maj_sum_high\n",
    "    \n",
    "def entropy(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
    "\n",
    "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
    "\n",
    "def gini_impurity(distr):\n",
    "    n = sum(distr.values())\n",
    "    ps = [n_i/n for n_i in distr.values()]\n",
    "    return 1-sum(p**2 for p in ps)\n",
    "    \n",
    "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
    "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then with using this newly defined class, we examine the optimal max depth for the tree using cross-validation. As well as the test accuracy using test data on the trained classifier.\n",
    "\n",
    "<span style=\"color:red\">Not sure which of the two cells below to use, i'd assume the one with the TreeClassifier, might have to ask tomorrow</span>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 10\n",
      "Test Accuracy: 0.9084507042253521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define the classifier\n",
    "cls = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'max_depth': range(1, 11)}  # Range of max_depth values to try\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=cls, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, Y) # Using the entire set\n",
    "\n",
    "# Get the best max_depth value\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "print(\"Best max_depth:\", best_max_depth)\n",
    "\n",
    "# Train the classifier with the best max_depth value on the entire training set\n",
    "best_cls = DecisionTreeClassifier(max_depth=best_max_depth, random_state=0)\n",
    "best_cls.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "Y_pred = best_cls.predict(Xtest)\n",
    "test_accuracy = accuracy_score(Ytest, Y_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 6\n",
      "Test Accuracy: 0.9131455399061033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the classifier\n",
    "cls = TreeClassifier(criterion='gini')\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'max_depth': range(1, 11)}  # Range of max_depth values to try\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=cls, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X, Y) # Using the entire set\n",
    "\n",
    "# Get the best max_depth value\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "print(\"Best max_depth:\", best_max_depth)\n",
    "\n",
    "# Train the classifier with the best max_depth value on the entire training set\n",
    "best_cls = DecisionTreeClassifier(max_depth=best_max_depth, criterion='gini')\n",
    "best_cls.fit(Xtrain, Ytrain)\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "Y_pred = best_cls.predict(Xtest)\n",
    "test_accuracy = accuracy_score(Ytest, Y_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that using the best performing max_depth of 6, we get a accuracy of 91,31%.\n",
    "\n",
    "Then by again using the TreeClassifier class we can display the tree using the draw_tree() function. If we set the max_depth to a lower number, in our case we picked 3, we can display an drawing of the decision tree that will fit in one page, which illustrates how the verdicts are constructed using thresholds and decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"689pt\" height=\"305pt\"\n",
       " viewBox=\"0.00 0.00 689.09 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-301 685.09,-301 685.09,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"43.55\" cy=\"-18\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"43.55\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"151.55\" cy=\"-18\" rx=\"46.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.55\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">suspect</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M197.05,-123C197.05,-123 106.05,-123 106.05,-123 100.05,-123 94.05,-117 94.05,-111 94.05,-111 94.05,-99 94.05,-99 94.05,-93 100.05,-87 106.05,-87 106.05,-87 197.05,-87 197.05,-87 203.05,-87 209.05,-93 209.05,-99 209.05,-99 209.05,-111 209.05,-111 209.05,-117 203.05,-123 197.05,-123\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.55\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">ASTV &gt; 59.5?</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M129.69,-86.8C112.69,-73.42 89.02,-54.79 70.78,-40.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72.86,-37.62 62.83,-34.18 68.53,-43.12 72.86,-37.62\"/>\n",
       "<text text-anchor=\"middle\" x=\"123.55\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M151.55,-86.8C151.55,-75.16 151.55,-59.55 151.55,-46.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"155.05,-46.18 151.55,-36.18 148.05,-46.18 155.05,-46.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"168.55\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"274.55\" cy=\"-18\" rx=\"59.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"274.55\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">pathologic</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"395.55\" cy=\"-18\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.55\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M370.55,-123C370.55,-123 304.55,-123 304.55,-123 298.55,-123 292.55,-117 292.55,-111 292.55,-111 292.55,-99 292.55,-99 292.55,-93 298.55,-87 304.55,-87 304.55,-87 370.55,-87 370.55,-87 376.55,-87 382.55,-93 382.55,-99 382.55,-99 382.55,-111 382.55,-111 382.55,-117 376.55,-123 370.55,-123\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.55\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">UC &gt; 3.5?</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M324.8,-86.8C315.69,-74.51 303.28,-57.77 293.08,-44.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"295.83,-41.84 287.06,-35.89 290.2,-46 295.83,-41.84\"/>\n",
       "<text text-anchor=\"middle\" x=\"329.55\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M349.28,-86.8C357.65,-74.54 369.02,-57.87 378.4,-44.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"381.48,-45.83 384.22,-35.6 375.69,-41.88 381.48,-45.83\"/>\n",
       "<text text-anchor=\"middle\" x=\"387.55\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M382.05,-210C382.05,-210 293.05,-210 293.05,-210 287.05,-210 281.05,-204 281.05,-198 281.05,-198 281.05,-186 281.05,-186 281.05,-180 287.05,-174 293.05,-174 293.05,-174 382.05,-174 382.05,-174 388.05,-174 394.05,-180 394.05,-186 394.05,-186 394.05,-198 394.05,-198 394.05,-204 388.05,-210 382.05,-210\"/>\n",
       "<text text-anchor=\"middle\" x=\"337.55\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">ALTV &gt; 68.5?</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>6&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M299.91,-173.8C270.57,-160.39 229.67,-141.7 198.24,-127.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.36,-124 188.81,-123.03 196.45,-130.37 199.36,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.55\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>6&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.55,-173.8C337.55,-162.16 337.55,-146.55 337.55,-133.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"341.05,-133.18 337.55,-123.18 334.05,-133.18 341.05,-133.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"354.55\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"516.55\" cy=\"-18\" rx=\"59.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.55\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">pathologic</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"637.55\" cy=\"-18\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"637.55\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M562.55,-123C562.55,-123 470.55,-123 470.55,-123 464.55,-123 458.55,-117 458.55,-111 458.55,-111 458.55,-99 458.55,-99 458.55,-93 464.55,-87 470.55,-87 470.55,-87 562.55,-87 562.55,-87 568.55,-87 574.55,-93 574.55,-99 574.55,-99 574.55,-111 574.55,-111 574.55,-117 568.55,-123 562.55,-123\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.55\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">Max &gt; 220.5?</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>9&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M516.55,-86.8C516.55,-75.16 516.55,-59.55 516.55,-46.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"520.05,-46.18 516.55,-36.18 513.05,-46.18 520.05,-46.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"535.55\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>9&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M541.03,-86.8C560.37,-73.22 587.42,-54.21 607.97,-39.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"610.16,-42.52 616.33,-33.9 606.14,-36.79 610.16,-42.52\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.55\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"636.55\" cy=\"-105\" rx=\"43.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"636.55\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">normal</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M567.05,-210C567.05,-210 466.05,-210 466.05,-210 460.05,-210 454.05,-204 454.05,-198 454.05,-198 454.05,-186 454.05,-186 454.05,-180 460.05,-174 466.05,-174 466.05,-174 567.05,-174 567.05,-174 573.05,-174 579.05,-180 579.05,-186 579.05,-186 579.05,-198 579.05,-198 579.05,-204 573.05,-210 567.05,-210\"/>\n",
       "<text text-anchor=\"middle\" x=\"516.55\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">Mean &gt; 107.5?</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>11&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M516.55,-173.8C516.55,-162.16 516.55,-146.55 516.55,-133.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"520.05,-133.18 516.55,-123.18 513.05,-133.18 520.05,-133.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"535.55\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>11&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M540.83,-173.8C560.01,-160.22 586.83,-141.21 607.21,-126.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"609.37,-129.54 615.51,-120.9 605.32,-123.83 609.37,-129.54\"/>\n",
       "<text text-anchor=\"middle\" x=\"601.55\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"yellow\" stroke=\"black\" d=\"M453.05,-297C453.05,-297 358.05,-297 358.05,-297 352.05,-297 346.05,-291 346.05,-285 346.05,-285 346.05,-273 346.05,-273 346.05,-267 352.05,-261 358.05,-261 358.05,-261 453.05,-261 453.05,-261 459.05,-261 465.05,-267 465.05,-273 465.05,-273 465.05,-285 465.05,-285 465.05,-291 459.05,-297 453.05,-297\"/>\n",
       "<text text-anchor=\"middle\" x=\"405.55\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">MSTV &gt; 0.55?</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;6 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>12&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M391.79,-260.8C382.02,-248.59 368.74,-231.99 357.77,-218.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"360.27,-215.8 351.29,-210.18 354.8,-220.17 360.27,-215.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"395.55\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;11 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>12&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M428.01,-260.8C444.71,-248.01 467.68,-230.42 486.04,-216.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"488.31,-219.03 494.12,-210.18 484.05,-213.48 488.31,-219.03\"/>\n",
       "<text text-anchor=\"middle\" x=\"485.55\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f5fbc64a080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls = TreeClassifier(max_depth=3, criterion='gini')\n",
    "cls.fit(Xtrain, Ytrain)\n",
    "cls.draw_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_doc</th>\n",
       "      <th>year</th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>num_room</th>\n",
       "      <th>kitch_sq</th>\n",
       "      <th>full_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25252</th>\n",
       "      <td>6150880</td>\n",
       "      <td>2014</td>\n",
       "      <td>61</td>\n",
       "      <td>32.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>247469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>6900000</td>\n",
       "      <td>2013</td>\n",
       "      <td>43</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>68630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18040</th>\n",
       "      <td>9600000</td>\n",
       "      <td>2014</td>\n",
       "      <td>56</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>78507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8625</th>\n",
       "      <td>10300000</td>\n",
       "      <td>2013</td>\n",
       "      <td>54</td>\n",
       "      <td>32.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>26943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13495</th>\n",
       "      <td>5000000</td>\n",
       "      <td>2013</td>\n",
       "      <td>38</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>132349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price_doc  year  full_sq  life_sq  floor  num_room  kitch_sq  full_all\n",
       "25252    6150880  2014       61     32.0    8.0       2.0      13.0    247469\n",
       "9943     6900000  2013       43     20.0   10.0       1.0       8.0     68630\n",
       "18040    9600000  2014       56     30.0   11.0       2.0       8.0     78507\n",
       "8625    10300000  2013       54     32.0   10.0       2.0       9.0     26943\n",
       "13495    5000000  2013       38     20.0    2.0       1.0       8.0    132349"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the CSV file using Pandas.\n",
    "alldata = pd.read_csv('Data/sberbank.csv')\n",
    "\n",
    "# Convert the timestamp string to an integer representing the year.\n",
    "def get_year(timestamp):\n",
    "    return int(timestamp[:4])\n",
    "alldata['year'] = alldata.timestamp.apply(get_year)\n",
    "\n",
    "# Select the 9 input columns and the output column.\n",
    "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
    "alldata = alldata[selected_columns]\n",
    "alldata = alldata.dropna()\n",
    "\n",
    "# Shuffle.\n",
    "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
    "\n",
    "# Separate the input and output columns.\n",
    "X2 = alldata_shuffled.drop('price_doc', axis=1)\n",
    "# For the output, we'll use the log of the sales price.\n",
    "Y2 = alldata_shuffled['price_doc'].apply(np.log)\n",
    "\n",
    "# Split into training and test sets.\n",
    "Xtrain2, Xtest2, Ytrain2, Ytest2 = train_test_split(X2, Y2, test_size=0.2, random_state=0)\n",
    "alldata_shuffled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted all suggested regressors without parameters and compared the test scores from cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             0           1\n",
      "0             DummyRegressor()   -0.391911\n",
      "1           LinearRegression()   -0.314657\n",
      "2                      Ridge()   -0.314656\n",
      "3                      Lasso()   -0.306771\n",
      "4      DecisionTreeRegressor()   -0.547681\n",
      "5      RandomForestRegressor()   -0.284441\n",
      "6  GradientBoostingRegressor()   -0.265231\n",
      "7               MLPRegressor() -473.534933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "dr = DummyRegressor()\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge()\n",
    "lasso = Lasso()\n",
    "dtr = DecisionTreeRegressor()\n",
    "rfr = RandomForestRegressor()\n",
    "gbr = GradientBoostingRegressor()\n",
    "mlpr = MLPRegressor()\n",
    "\n",
    "models = [dr, lr, ridge, lasso, dtr, rfr, gbr, mlpr]\n",
    "a = []\n",
    "for x in models:\n",
    "    mean = cross_validate(x, X2, Y2, scoring='neg_mean_squared_error')['test_score'].mean()\n",
    "    a= np.append(a,mean)\n",
    "\n",
    "df = pd.DataFrame(list(zip(models,a)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we decided to pick the one with the lowest negative mean squarre error, namely Gradient boosting regressor.\n",
    "\n",
    "Now to optimize this regression model, we started off with what was familiar, looking for a max depth which would yield the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_depth: 5\n"
     ]
    }
   ],
   "source": [
    "regr = GradientBoostingRegressor()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'max_depth': range(1, 11)}  # Range of max_depth values to try\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=regr, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X2, Y2) # Using the entire set\n",
    "\n",
    "# Get the best max_depth value\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "print(\"Best max_depth:\", best_max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_depth=5, random_state=0 -> 0.26405492251107027"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0         1\n",
      "0  1.00 -0.336091\n",
      "1  0.75 -0.309303\n",
      "2  0.50 -0.275331\n",
      "3  0.25 -0.260040\n",
      "4  0.00 -0.391911\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [1, 0.75, 0.5, 0.25, 0.0]\n",
    "a = []\n",
    "for learn_r in learning_rates:\n",
    "    regr_test = GradientBoostingRegressor(max_depth=5, random_state=0, learning_rate=learn_r)\n",
    "    mean = cross_validate(regr_test, X2, Y2, scoring='neg_mean_squared_error')['test_score'].mean()\n",
    "    a = np.append(a,mean)\n",
    "\n",
    "df = pd.DataFrame(list(zip(learning_rates,a)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0         1\n",
      "0    1 -0.343485\n",
      "1    2 -0.315292\n",
      "2    4 -0.288250\n",
      "3    8 -0.273365\n",
      "4   16 -0.266031\n",
      "5   32 -0.260609\n",
      "6   64 -0.257910\n",
      "7  100 -0.260040\n",
      "8  200 -0.269079\n"
     ]
    }
   ],
   "source": [
    "estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n",
    "a = []\n",
    "for n_est in estimators:\n",
    "    regr_test = GradientBoostingRegressor(max_depth=5, random_state=0, learning_rate=0.25, n_estimators=n_est)\n",
    "    mean = cross_validate(regr_test, X2, Y2, scoring='neg_mean_squared_error')['test_score'].mean()\n",
    "    a = np.append(a,mean)\n",
    "\n",
    "df = pd.DataFrame(list(zip(estimators,a)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = GradientBoostingRegressor(max_depth=5, random_state=0, learning_rate=0.25, n_estimators=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26476194648505785"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "  \n",
    "regr.fit(Xtrain2, Ytrain2)\n",
    "mean_squared_error(Ytest2, regr.predict(Xtest2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
